# Method I：在线搜索（NewAgent）实现总结（基于当前 agent.py）

本文档总结当前 `agent.py` 中 `NewAgent` 的“在线搜索 / 在线规划”方法实现思路与关键细节，供直接改写到报告 Method I。

> 核心定位：**不用训练模型**，每回合在当前局面中“生成候选 → 物理仿真评估 → 噪声鲁棒性筛选 → 局部微调 → 安全兜底”。

---

## 1. 问题与约束

- 环境输入：`(balls, my_targets, table)`。
- 连续动作：`(V0, phi, theta, a, b)`。
- 真实执行带噪声：环境会对动作参数加入小高斯扰动，因此**单次仿真高分不等于实战可靠**。
- 规则关键：
  - 清台前黑 8 不可进袋；清台后必须合法打进 8。
  - 必须先击打己方目标球（或仅剩 8 时先击打 8）。
  - 未进球时需要碰库（实现里采用“白球或首碰球碰库”的近似判定）。

---

## 2. 总体框架（decision 主流程）

`NewAgent.decision()` 的决策是两段式：

1. **进攻搜索**：`_find_best_shot()` 先用几何+仿真在“目标球×袋口”组合中找一个最好的基础方案（base_action）。
2. **局部微调**：`_refine_shot_with_simulation()` 对 base_action 做局部随机扰动搜索（30 次），用鲁棒评分选更稳的 refined_action。

若进攻搜索/微调后的鲁棒得分仍偏低（阈值约 -80），则进入：

3. **防守/保底**：`_get_safe_shot()` 生成“能合法首碰且尽量不送命”的安全球；如果安全球仍很差，则尝试 `Kick Shot / Bank Shot`（大力解球策略）。

最后还会再做一次**首球合法性快速验证**，不合法直接切回安全策略。

---

## 3. 候选生成：目标球 × 袋口 + Ghost Ball

### 3.1 袋口位置
- `table.pockets.values()` 取每个袋口中心 `pocket.center`，只用 2D `(x, y)`。

### 3.2 Ghost Ball（瞄准点）
- 对每个 `(target_id, pocket)`：
  - 计算 target→pocket 的方向单位向量。
  - 反推“母球应到达的碰撞点”（ghost ball）
    
    \[ \text{aim\_point} = \text{target} + \hat{d}\cdot(2R) \]

  其中 \(\hat{d}\) 是从袋口指向目标球的单位方向，\(R\) 为球半径（实现里 `BALL_RADIUS=0.028575`）。

### 3.3 由几何解得到初始动作
- `phi`：`cue_pos → aim_point` 的方向角（`atan2` 转度数，归一到 `[0, 360)`）。
- `V0`：按 “母球→aim_point” 距离 + “目标球→袋口” 距离 的分段经验式给定，并整体乘上 `POWER_REDUCTION=0.9` 以降低白球进袋风险。
- 初始默认：`theta=0, a=b=0`。

---

## 4. 几何启发式评分（快速筛）

`_evaluate_shot_quality()` 给每个候选一个几何分 `geo_score`，用于减少仿真开销并做综合排序。其主要项：

- 距离惩罚：总距离越大越扣分。
- 切角难度：用 `cue→aim_point` 和 `target→pocket` 的夹角，直球奖励，大角度惩罚。
- 路径遮挡：
  - `cue→aim_point`、`target→pocket` 两段做“线段附近是否有球”的简化检测（阈值约 `2.2R`）。
- 袋口接近奖励：目标球离袋口越近越加分。
- 规则/风险项：
  - 用 `_get_first_contact_ball()` 预测首碰球，不是目标球则大幅惩罚（尤其该打 8 时）。
  - 黑 8 风险：非打 8 时，如果击球路线可能触碰 8 或目标球入袋路线可能带入 8，则额外惩罚。

> 这一步是**启发式剪枝**：并不保证可行，只是减少“明显离谱”的候选进入昂贵的仿真阶段。

---

## 5. 首球预测（用于快速合法性过滤）

`_get_first_contact_ball(cue_pos, phi, balls)`：用纯几何近似预测白球沿 `phi` 射线最先撞到谁。

- 对每颗未进袋球：
  - 计算其在射线方向上的投影距离 `proj`（必须 > 0）。
  - 计算到射线的垂距 `perp_dist`。
  - 若 `perp_dist < 2R`，可发生碰撞；用勾股推算碰撞点距离 `collision_dist = proj - sqrt((2R)^2 - perp_dist^2)`。
- 取 `collision_dist` 最小者作为首碰球。

用途：
- 在进入仿真前就过滤掉明显首球犯规的局部扰动参数，节省仿真预算。
- 在最终决策前再做一次快速验证，避免输出明显非法动作。

---

## 6. 仿真评估与规则对齐评分

### 6.1 物理仿真
- `_simulate_and_evaluate()` 会对 `balls/table` 深拷贝，构造 `pt.System`，设置球杆参数后调用 `simulate_with_timeout()`。
- `simulate_with_timeout()` 在支持 `signal.SIGALRM` 的平台（通常 Unix/Linux）上提供硬超时；在 Windows / 不支持 SIGALRM / 非主线程等情况下会自动**降级为直接 `pt.simulate`**，避免因信号机制不可用导致评测失败。

### 6.2 评分函数：强约束“一票否决” + 强罚犯规

仿真结束后通过 `shot.balls` 的状态变化得到 `new_pocketed`，并解析 `shot.events` 判断首球、碰库等。

主要规则：

- **一票否决（致命错误）**：
  - 白球 + 黑 8 同进：直接 `-10000`。
  - 清台前打进黑 8：直接 `-10000`。
- **高风险错误（强惩罚）**：
  - 白球进袋：`-1000`。
  - 首球犯规：约 `-180`；没碰到任何球：约 `-120`。
  - 未进球且未碰库：约 `-200`。
- **进球收益**：
  - 己方球进袋：每颗 `+100`。
  - 对手球进袋：每颗 `-40`。
- **合法无进球奖励**：`+10`。
- **袋口斥力场**：如果白球最终停在袋口很近（`<1.5R`），额外扣分（最多约 500 级别），以避免“停在袋口边缘下一杆容易 scratch”。

> 该评分设计的意图是：与其追求高风险高回报，不如优先保证合法与避免直接输局事件，从而提高对战胜率。

---

## 7. 鲁棒性评估（噪声蒙特卡洛）

`_evaluate_with_robustness(action)` 是在线搜索方法的关键（风险厌恶）：

- 对同一动作重复多次带噪声仿真（默认 FINAL=10），每次在 `V0/phi/theta/a/b` 上加与环境一致的高斯噪声（`noise_std`）。
- **早停（加速失败候选剔除）**：一旦任意一次出现一票否决级错误（`score <= -5000`），立刻停止剩余采样并判为不可用（返回 `-10000`）。
- 否则用保守聚合：

  \[ \text{robust\_score} = 0.6\,\mathbb{E}[s] + 0.4\,\min(s) \]

- 再对“高风险/中风险错误次数”做额外惩罚（阈值随采样数自适应），进一步抑制不稳定动作。

这一步本质是 **chance constraint / risk-averse**：用 Monte Carlo 近似估计“在执行噪声下的最坏情况”，优先选稳定球。

---

## 8. 组合策略：几何分 + 鲁棒分

在 `_find_best_shot()` 中，对每个 `(target, pocket)`：

- 先算 `geo_score`（快）。
- **两阶段评估（提速关键）**：
  - Stage A：先对候选动作做一次“无噪声单次仿真”快速筛选（`sim_score_fast`），剔除明显离谱/致命候选。
  - Stage B：只对 Top-K（例如 6 个）候选做全量带噪声鲁棒评估（`sim_score`），把仿真预算集中到最有希望的方案上。
- 最终综合：

  \[ \text{total} = 0.3\,\text{geo\_score} + 0.7\,\text{sim\_score} \]

权重设置体现“最终还是以仿真鲁棒性为主”，几何仅作为先验。

---

## 8.1 工程加速改进（2025-12-19 更新）

本次对工作区版本进行了针对性加速，目标是在**不牺牲最终动作评估精度**的前提下降低每回合仿真次数：

- 鲁棒性评估分为两档采样数：
  - `ROBUSTNESS_SAMPLES_SCREEN`（默认 4）：用于搜索/微调阶段快速筛选。
  - `ROBUSTNESS_SAMPLES_FINAL`（默认 10）：用于最终候选复核与输出动作评分，保持最终精度。
- 候选集合只对 Top-K（`ROBUST_TOPK`，默认 6）做 FINAL 级别鲁棒评估。
- 微调阶段先用 SCREEN 级别鲁棒得分挑最佳，再对最佳动作做 FINAL 复核后返回。

这一改动借鉴了“先 cheap 评估筛候选、再重评估确认”的思想，通常能显著减少 total simulation 数量。

---

## 8.2 开球快速策略（2025-12-19 更新）

问题：当 `NewAgent` 作为 Player A 执行**第一杆开球**时，若仍按常规流程跑“候选生成 → Top-K 全量鲁棒 → 微调”，会在开球局面浪费大量仿真时间；而开球动作本身不需要如此精细的在线规划。

解决：在 `NewAgent.decision()` 的开头加入“开球局面检测”，一旦判定为开球，直接走轻量开球策略而不进入完整在线搜索。

### 8.2.1 开球局面检测（`_is_break_state`）

由于环境不会把 `hit_count` 传给 Agent，这里采用几何形态推断：

- 15 颗彩球（1~15）全部仍在台面（未进袋）。
- 彩球位置呈现明显的三角架聚类：
  - bounding box 对角线 `bbox_diag` 较小；
  - 相对聚类中心的最大半径 `max_r` 较小。
- 白球与聚类中心距离 `cue_to_centroid` 较大（典型开球位）。
- 额外限制：`my_targets != ['8']`（排除“只剩黑8”的终局形态误判）。

以上条件满足时，认为是“刚开局的开球局面”。

### 8.2.2 轻量开球决策（`_break_decision`）

核心思想：只做少量候选、极少评估，但仍尽量保证规则合法。

- 先计算白球指向球团质心的角度 `base_phi`。
- 在 `base_phi` 附近尝试少量角度偏移 `delta`，用几何首碰预测 `_get_first_contact_ball()` 选择**首碰球落在 `my_targets` 内**的角度：
  - 避免“条纹方开球先碰到实心球”等首球犯规。
  - 同时禁止“先碰 8 号球”（在清台前）。
- 在该角度下只枚举少量力度候选（例如 V0=5.8/6.4/7.0，`theta=a=b=0`）。
- 评估方式极简：
  - 先做 1 次无噪声仿真快速筛掉致命候选；
  - 不做噪声鲁棒评估；仅用这一次仿真得分在候选中选最优。

收益：开球从“高仿真预算”降为“常数级小预算”，显著减少开局回合的等待时间。

---

## 9. 局部微调（Local Random Search）

`_refine_shot_with_simulation(base_action)` 做 30 次局部采样：

- `V0`: `±0.5`，再 clip 到 `[1.2, 5.5]`（更保守的速度范围）。
- `phi`: `±4` 度。
- `theta`: 采样 `[0, 12]`，再 clip 到 `[0, 15]`。
- `a,b`: 采样 `[-0.12, 0.12]`。

并且在每次仿真前用 `_get_first_contact_ball()` 做首球合法性快速过滤（不合法直接跳过），从而把仿真预算集中在可能有效的区域。

补充（2025-12-19）：微调阶段使用 SCREEN 级鲁棒评估做筛选，最后对最佳动作做 FINAL 复核后输出，以兼顾速度与精度。

---

## 10. 防守与绝境策略

当进攻搜索失败或得分过低，进入 `_get_safe_shot()`：

- 尝试对每个目标球：
  - 直接瞄准目标球中心（或微调角度绕障）。
  - 速度设为能确保碰库的区间（`2.5~4.0` 左右，随距离调节）。
  - 用鲁棒评估选最好。
- 若仍不行：遍历 `phi` 每 15 度尝试首碰合法球。
- 仍失败：选择“最近的目标球”做最小风险击打，并做鲁棒检查。

若安全球的最佳得分仍低于防守底线（`SAFE_SHOT_THRESHOLD=-50`），则尝试 `_try_kick_shot()`：

- Kick Shot：对合法首碰的目标球用较大力度（`V0=5.0~6.5`）+ 小角度扰动制造复杂局面，但仍用鲁棒评估筛掉致命错误。
- Bank Shot：朝袋口方向加大角度偏移（约 ±15~25 度）以确保碰库，尝试通过反弹解决卡球。

---

## 11. 超参数与工程取舍（可写进报告）

- `SIMULATION_TIMEOUT=2s`：单次仿真超时保护（平台相关）。
- `ROBUSTNESS_SAMPLES_FINAL=10` / `ROBUSTNESS_SAMPLES_SCREEN=4`：鲁棒评估采样数（最终/筛选），越大越稳但越慢。
- `ROBUST_TOPK=6`：只对 Top-K 候选做 FINAL 评估，控制总仿真预算。
- `SIMULATION_COUNT=30`：局部微调次数，越大越可能找到更优解但越慢。
- 综合权重：`0.3*geo + 0.7*sim`：强调“仿真 + 鲁棒”才决定最终。

**优点**：几乎不依赖训练；对规则犯规的抑制强，实战胜率常高。

**缺点**：推理时间随仿真次数增长；评分函数/阈值较启发式；超时机制在 Windows 上不可用（若评测系统为 Linux 则无影响）。
